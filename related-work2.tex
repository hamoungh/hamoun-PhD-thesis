

 \chapter{Related Work}      
\label{ch:state-of-art}    
%\begin{center}
%\textbf{This chapter contains material from Ghanbari et al. \cite{hamoun_ghanbari_exploring_2011,hamoun_ghanbari_tuning,ghanbari2014mpc,ghanbari_feedback-based_????}}.
%\end{center}

The chapter reviews the current state of the art for solutions to the IT service resource management problems introduced in chapter 1.   
The problem solutions reviewed include:
   (i) methods for reducing monitoring overhead and computational complexity of the Bayesian service demand estimation for service replicas,
   (ii) methods for improving the placement of service replicas in a private cloud using dynamic empirical models, and 
   (iii) methods for considering the reconfiguration cost, when modeling the optimal placement of the service replicas.
 
   Separate sections are provided which describe the related work for each (i-iii) of these problems. 

 \section{Service Demand Estimation}   
\label{sec:service_demand_estimation} 

Some early research into CPU demand estimation is described in \cite{menasce2008computing}.
  % this is just equation solving for each step separately, Computing Missing Service Demand Parameters for Performance Models
In this early work, missing service demands are computed by solving a set of nonlinear equations based on a queuing model. For each step, the following equations are solved:
 \begin{align}
 \underset{(d_{c,k})} {\text{minimize } }   & 
 || R_c-\sum_{k=1}^K \frac{d_{c,k}}{1-\sum_{c'=1}^C \lambda_{c'}d_{c',k}}||  \nonumber \\  % \sum_{c=1}^{C}  
 \text{subject to:}  \label{eq:simplistic_estimation} \\ 
 &  d_{c,k}>0  \text{  for each $c$,$k$} \nonumber \\ 
 & \sum_{c=1}^C \lambda_{c} d_{c,k} < 1 \text{  for each $k$} \nonumber
 \end{align}  
 where $R_c$ and $\lambda_c$ are known and $D_{c,k}$ are estimated.  
 To solve \ref{eq:simplistic_estimation}, Menasc{\'e} et al. use an iterative approximation algorithm that takes the estimated demands of the past step as the initial value for the estimates of the current step.

\cite{rolia_parameter_1995} and \cite{rolia_correlating_1998} used linear regression to predict the demands of running programs in a distributed system. Many works, such as \cite{pacifici_cpu_2008}, estimate service demands by solving the following linear Least Square Estimation (LSE) problem based on the utilization law %\ref{eq:utilization-law}: 
  \begin{align}     \label{eq:utilization_based_estimation}
    \underset{(d_{c,k})} {\text{minimize } }   ||U_{t,k} -  \sum_{c} X_{c,t}  d_{c,k}||   
  \end{align} 
 % $\ddot{\utilization_{h,k',t'}}  =  \sum_s \ddot{X_{s,h,t}} \hat{D_{s,k'}}$
%  Thus the solution for all centres can be obtained by: 
% \[ D =  ( \lambda^T \lambda)^{-1} \lambda^T U \] 
% \[ U_t=\sum_{c=1,...,C} \lambda_{c} D_{c,k} \]      
  here $U_{t,k}$ denotes the utilization of the $k$'th resource centre at time $t$, $X_c$ denotes the throughput of the customer class $c$, $d_{c,k}$ denotes the class $c$'s service demand on the $k$'th resource centre, and $||.||$  represents the second norm function. In the formulation \ref{eq:utilization_based_estimation}, it is assumed that the throughput values of all user classes and the utilizations of all servers are known over some time interval. It is also assumed that, the LSE is based on the utilization law and the assumption that residuals are IID noise with a Gaussian density (i.e. $N(0, \sigma^2)$): 
 \begin{align}
 U_{t,k} = \sum_{c} X_{c,t}  d_{c,k} + v_{t,k},\   t=1,...,T , k=1,...,K 
  \end{align}
 where $T$ is the length of a monitored time interval (or the number of samples), $K$ is the number of resources and $v_{t,k}$ is the residual associated with time $t$ and resource $k$.   

 References \cite{pacifici_cpu_2008,zhang_regression-based_2007} employ the multivariate linear regression technique for a one-tier network. However, \cite{zhang_regression-based_2007} focuses more on modeling inter-request dependencies of session-based systems and \cite{pacifici_cpu_2008} focuses on the mechanisms to deal with the issues such as insignificant flows, collinear flows, space and temporal variations, and background noise. 
 
 \subsection{Non-Linear Regression}
If response time metrics are available, the service demands problem can be posed as a nonlinear least squares estimation. 
  An example of research where regression splines are utilized is \cite{courtois_using_2000}. To find model parameters in nonlinear least squares problems, usually algorithms such as Levenberg-Marquardt \cite{wild_nonlinear_2003,dumouchel1989integrating} can be used. In \cite{dumouchel1989integrating}, for example, a weighted nonlinear regression is iteratively refined. Weights used in each
  iteration are based on observation residuals from the previous iteration. Points that are outliers are down-weighted, and their influence on the fit is decreased. Iterations continue until the weights converge. Note that, the convergence of the parameters and the correctness of the estimation depend on the underlying model. If applying MLE on the model does not follow a convex form, it is quite possible that the regression does not converge, or converges to a wrong value.  
 % \cite{rolia_parameter_1995,zhang_regression-based_2007,rolia_correlating_1998}: 
% 
	In \cite{zhang_workload_2002} and \cite{liu_parameter_2006}, multi-class queuing models were used in a two-tier web cluster, to infer service times per-class at different servers using throughput, utilization, and per-class response time measurements. They try to minimize the sum of predicted response time mean square errors using a non-linear optimization solver and a quadratic minimization program. \cite{kraft_estimating_2009} also uses a combination of MLE and the queuing model for estimation.  

 References \cite{gmach_workload_2007,gmach_capacity_2007} claim to perform the demand prediction of enterprise workloads by discovering patterns, but the referred demand is per-application not per-request; thus, they do not tackle the same problem as ours. 
 
   Despite simplicity and ease of implementation, the regression-based approaches have three major shortcomings:
   \begin{enumerate}
   \item  In the presence of missing measurements, the regression problem might not be a convex optimization. For example, in the case where measurements are simply throughput and utilization, if a resource utilization or a class throughput is missing, terms such as $X_{c,t}.d_{c,k}$ might make the optimization non-convex.   
% also an additional $T$ variables are added to the optimization. 
   \item  Even if the solution to the LSE problem can be formulated analytically (i.e. through algebraic operations on matrices), the computational complexity is still third-degree polynomial with respect to the number of samples $T$ and resources $K$: $O((T.K)^3)$. Thus, as the size of regression window grows (i.e. to provide more accuracy), solving for a large-scale environment (cloud) becomes more computationally expensive.  
   \item There is no way to specify the rate of change in the unknown variables explicitly. The value of unknown demands is considered constant within the regression interval. As a result, the number of samples within each steady-state interval determines how quickly the estimated demands are going to change with time. With fewer samples, they will change more frequently and with more samples they will change less. 
     \end{enumerate}

    \subsection{Bayesian Approach}   
   %  The Bayesian posterior estimation was thoroughly described in \ref{sec:bayesian-estimation}.
		Papers \cite{woodside_use_2005,xu_performance_2005,zheng_tracking_2005} use Bayesian estimation to effectively deal with the problems of regression-based approaches enumerated earlier. They treat all unknown parameters as variables that vary over time. Thus, they can deal with the addition of a missing data to unknown parameters without extra complexity. They also reduce the amount of computation by solving the associated problem analytically and deriving a
        filter for it (i.e. Kalman type). This filter greatly reduces the estimation cost by doing step-by-step computation based only on the measurements of the current step and the state of the previous step, thus reducing computations to $O((C\cdot K)^3)$. Using a Kalman filter, they also specify the rate of change of the unknown parameters by specifying the process noise and the measurement noise covariance matrices. 

        The only issue in using the Bayesian approach is convergence. Convergence requires that at minimum, we have more measured parameters than estimated ones. As will be discussed in chapter \ref{ch:estimation} in detail, this convergence condition either puts a limit on the number of classes whose demands are to be estimated or requires a considerable number of parameters to be measured. The major contribution of our approach is dynamic clustering of user
  classes, to minimize the number of unknown variables. Yet, we leave enough classes to satisfy the accuracy criterion.  

  To the best of our knowledge, the combination of dynamic clustering and performance model parameter estimation for multi-class models has never been investigated. The only close reference to our work in terms of the final goal is \cite{sharma_automatic_2008}. 
In \cite{sharma_automatic_2008}, Sharma et al. try to categorize requests based on resource usage characteristics. However, \cite{sharma_automatic_2008} ignores the prior knowledge about requests in the system and classify the requests on-the-fly. Thus, their utilized approach is also different. They use a machine learning technique called Independent Component Analysis (ICA). Of course, ICA will alleviates the need for server instrumentation, but might affect the accuracy due to not using the extra available information (i.e. of the classes of users).


\section{Application Resource Share Adjustment in Cloud Using Dynamic Empirical Models}      
\label{sec:application_resource_share_optimization_private_cloud}
 	%As examples of this contribution, \cite{li_fast_2009,li_performance_2009} attempt to find the minimum infrastructure cost deployment, subject to processing capacities and application SLAs. SLAs are described in terms of constraints on the average delay and throughput for each application.  
%\cite{li_sla-driven_2010} attempts to simultaneously minimize cost while maximizing QoS attributes, through multi-objective optimization or MOO. A Pareto-optimal solution can find a good trade-off between conflicting performance and cost-saving goals rather than finding a single global optimum~\cite{soror_automatic_2010}. Geometrically these well-balanced solutions concentrate around the ``knee'' of a multi-objective trade-off curve. 

Maintaining application level QoS  guarantees has been the
subject of much investigation. Recently satisfying the dual objectives of
QoS guarantees and server consolidation in cloud computing environments has
received considerable attention. Current approaches formulate the optimization
problem in different forms.

One approach attempts to minimize cost subject to performance constraints
\cite{li_fast_2009,li_performance_2009}. In this approach, the SLA represents
constraints rather than flexible goals. Constraints could be based on response
time or throughput. For example \cite{li_fast_2009} finds the minimum cost
deployment subject to processing capacity and user throughput constraints. It
seeks deployments which minimizes the overall cost of the hosts used, subject to meeting average delay and
throughput constraints for each application as posed by its SLA. 

The second approach attempts to minimize cost while maximizing QoS
attributes simultaneously, through multi-objective optimization or
MOO~\cite{li_sla-driven_2010}. For example, Pareto-optimal solutions can find a
good trade-off between conflicting performance and cost-saving goals rather than
 finding a single global optimum~\cite{soror_automatic_2010}. Geometrically,
 these well-balanced solutions concentrate around the ``knee'' of a
 multi-objective curve.

 % The purpose of this work was to demonstrate the advantage of ``adaptive'' models relative to 
% ``static'' models in optimization.
The main difference between our approach the related work is the use of dynamic empirical models (for each application) within the
global optimization loop. These models update themselves at runtime in order to adapt to perturbations in the environment not captured in initial model specification.  This results in more accurate models being passed to the optimizer, allowing for better resource utilization on a global scale.  

% huse of the decomposed model for better convergence, a custom optimization routine, and the use of utility functions. The works \cite{li_fast_2009,li_performance_2009} find the minimum cost deployment subject to processing capacity and user throughput constraints. However, they assume a monolithic model has already been built.
% \cite{soror_automatic_2010} targets a Pareto-optimal solution, a good trade-off between conflicting performance and cost-saving goals. Also the idea of dynamically adjusting the resource shares of multiple applications in order to meet a specified level of service differentiation, was used in \cite{liu_optimal_2007} and \cite{lim_automated_2009}, although using adaptive multivariate controller and for a more limited scenario than ours (i.e. maximum one PM per application layer).
    
\section{Service Replica Placement Considering the Reconfiguration Cost} 
\label{sec:service_replica_placement_considering_trhing_cost} 
 Static or one step ahead optimizations are the main tool to target minimization of infrastructure cost for private clouds while meeting applications QoS guarantees. But, when one takes into account the cost of reconfiguration, one has to add the time dimension into the optimization. This makes the optimization very complicated, and this complexity should be dealt with using the proper method. The approach we take here is using the Model Predictive Control (MPC) framework.  

In terms of related work, there are only a couple of papers that  target the same kind of problem.
Paper \cite{zhang2012dynamicPlacement} discusses the service placement in geographically distributed clouds through MPC. However, the approach~\cite{zhang2012dynamicPlacement} defers from our approach in two ways: (i) it uses a single class open queuing network model, which is much simpler than the close multi-class model we use, and (ii) it derives an analytical solution to a SSEC version of the problem first and then targets this solution as a desired state through MPC, considering the quadratic reconfiguration stage cost. In our case, this was not possible because we needed a sparse deployment at each step. From the same authors, paper \cite{zhang2012dynamicProvisioning} discusses controlling the optimal number of active servers for Google cluster where web applications and data processing jobs are both considered as generic jobs treated under scheduling policies. In this paper, they also use the MPC, similar to our approach. They first found the optimal steady-state solution through a single class queuing model and then reached a solution considering the reconfiguration cost through MPC.

Other examples of using MPC in computing-resource management context are~\cite{baiefficient,abdelwahed2004control, kandasamy2004self, bhat2006enabling,patikirikorala2011hammerstein}; however, these examples do not target service placement.
In \cite{baiefficient,abdelwahed2004control,kandasamy2004self} MPC is used for managing web server power consumption by changing the frequency at which the CPU is operating. 
In this example, the number of choices of frequencies is quite small (i.e. less than 10 alternatives), changes in the frequency are trivial, and the feedback does not have a lot of delay. 
They were able to use a very small lookahead horizon and a simple model, and calculate the actual expected value of the cost over process noise ($w_j$) distribution, for every action path, without getting involved in a state explosion.  
In \cite{bhat2006enabling} MPC is used to decide about the amount of data each node of a cluster should send through a set of data streams, and the amount to cache to disks. The proposed solution maximizes the throughput up to the network congestion point. In this paper the cost function was quadratic, which greatly aids with solving the MPC problem (heuristics not needed).   

  \section{Summary}   
 In this chapter, we provided the related work for the problems we introduced in chapter 1. The first problem we targeted is scaling up the EKF based estimation of service demands for large-scale service centres without increasing the monitoring overhead. Currently, there are two sets of techniques for service demand estimation:  regression-based approaches and Bayesian filtering approaches. The Bayesian approach addresses the existing issues with regression-based approaches, such as  the ability to tune the expected rate of parameter changes over time.
 However, Bayesian approach suffers from a lack of convergence when the number of missing data items and hidden parameters exceeds the number of monitored metrics. Our proposal improves the Bayesian estimation in terms of convergence. It reduces the proportion of unknown parameters and metrics to measured ones by grouping user classes, giving the filter a chance to converge with a less monitoring overhead, and less computational complexity. 
 
 The second problem we target in this thesis is using dynamically tuning to improve the accuracy of the empirical models; we then trace this accuracy in the overall performance improvement in the allocation. We try to adjust the resource shares of a set of virtual machines that run on behalf of a set of applications. These VMs are deployed on a limited number of physical machines. The goal is to best satisfy the overall application performance, performance being
 described in terms of a set of utility functions. Our approach was based on point-wise stationary approximation (PSA) and solved for a set of the steps separately.
	   
	The third problem was the placement of services considering the reconfiguration cost. MPC is used to derive an optimal action at any time step based on the amount of workload to the services experienced to that point. This let us take the reconfiguration cost into account and minimize it.


